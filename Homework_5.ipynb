{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "from operator import itemgetter\n",
    "import networkx as nx\n",
    "import statistics\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the given data into data structures that we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary of categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {}\n",
    "with open('wiki-topcats-categories.txt', 'r') as f:    \n",
    "    for i in f:\n",
    "        i = i.strip('').lstrip('Category:').replace(';','').split()\n",
    "        if len(i)-1 >= 3500:\n",
    "            key = i[0]\n",
    "            value = list(map(int, i[1:]))\n",
    "            categories.update({key:value})\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the list of page names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki-topcats-page-names.txt', 'r') as f:\n",
    "    pagenames = [' '.join(i.split()) for i in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 Chiasmal syndrome',\n",
       " '1 Kleroterion',\n",
       " '2 Pinakion',\n",
       " '3 LyndonHochschildSerre spectral sequence',\n",
       " \"4 Zariski's main theorem\",\n",
       " '5 FultonHansen connectedness theorem',\n",
       " \"6 Cayley's ruled cubic surface\",\n",
       " '7 Annulus theorem',\n",
       " \"8 Bing's recognition theorem\",\n",
       " '9 BochnerMartinelli formula']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagenames[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested list of connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki-topcats-reduced.txt', 'r') as f:\n",
    "    connections = [tuple(map(int, i.strip().split())) for i in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(52, 401135),\n",
       " (52, 1069112),\n",
       " (52, 1163551),\n",
       " (62, 12162),\n",
       " (62, 167659),\n",
       " (62, 279122),\n",
       " (62, 1089199),\n",
       " (62, 1354553),\n",
       " (62, 1400636),\n",
       " (62, 1403619)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connections[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research questions\n",
    "# RQ1\n",
    "### Constructing directed graph dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = defaultdict(set)\n",
    "for i in connections:\n",
    "    graph[i[0]].add(i[1])\n",
    "    if i[1] not in graph:\n",
    "        graph[i[1]] = set()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the graph <br>\n",
    "### We are aware that there is a faster way to build the graph by only one function given by networkX, but we want <br> <br>to add attributes for the nodes, thus we build the graph by normal way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in graph.items():\n",
    "    G.add_node(key)\n",
    "    for attr in value:\n",
    "        G.node[key][attr] = pagenames[attr] # insert page name as attribute for each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for connection in connections:\n",
    "    G.add_edge(connection[0], connection[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the graph (Answers for RQ1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: \\nType: DiGraph\\nNumber of nodes: 461193\\nNumber of edges: 2645247\\nAverage in degree:   5.7357\\nAverage out degree:   5.7357'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.info(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2436602635647606e-05"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.density(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the graph is not dense as the density is very low.<br/>\n",
    "We can also check a node is connected to others and by their names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1537692: '1537692 American University',\n",
       " 12162: '12162 Oliver Sacks',\n",
       " 1403619: '1403619 James Brady',\n",
       " 1544420: '1544420 Tim Tebow',\n",
       " 167659: '167659 Dmitri Nabokov',\n",
       " 1089199: '1089199 David Eagleman',\n",
       " 279122: '279122 United States',\n",
       " 1354553: '1354553 Jay Hambidge',\n",
       " 1400636: '1400636 Ronald Reagan'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.node[62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_dict = dict(G.adjacency()) # adjacency dictionary of the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of articles in the reduced list\n",
    "set_reduce = set([i for j in connections for i in j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461193"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set_reduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we only consider those categories with number of connected nodes >= 3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English_footballers 6884\n",
      "The_Football_League_players 7232\n",
      "Association_football_forwards 4615\n",
      "Association_football_goalkeepers 3664\n",
      "Association_football_midfielders 5282\n",
      "Association_football_defenders 4126\n",
      "Living_people 323384\n",
      "Harvard_University_alumni 5262\n",
      "Major_League_Baseball_pitchers 4803\n",
      "Members_of_the_United_Kingdom_Parliament_for_English_constituencies 6398\n",
      "Indian_films 5438\n",
      "Year_of_death_missing 3629\n",
      "Year_of_birth_missing_(living_people) 26058\n",
      "Rivers_of_Romania 7729\n",
      "Main_Belt_asteroids 11238\n",
      "Asteroids_named_for_people 4700\n",
      "English-language_albums 4730\n",
      "British_films 4382\n",
      "English-language_films 22358\n",
      "American_films 15090\n",
      "People_from_New_York_City 4480\n",
      "American_television_actors 11409\n",
      "American_film_actors 13772\n",
      "Debut_albums 7320\n",
      "Black-and-white_films 10586\n",
      "Year_of_birth_missing 3908\n",
      "Place_of_birth_missing_(living_people) 5092\n",
      "American_military_personnel_of_World_War_II 3639\n",
      "Windows_games 3774\n"
     ]
    }
   ],
   "source": [
    "reduce_categories = {}\n",
    "for category, articles in categories.items():\n",
    "    set_articles = set(articles)\n",
    "    intersect = set_reduce.intersection(set_articles) # only taking the nodes inside the reduce graph\n",
    "    intersect = list(intersect)\n",
    "    for article in intersect:\n",
    "        if len(list(G.neighbors(article))) == 0: # remove the nodes that are not connected\n",
    "            intersect.remove(article)\n",
    "    if len(intersect) >= 3500:\n",
    "        reduce_categories[category] = intersect\n",
    "        print(category,len(intersect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reduce_categories.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1048576,\n",
       " 1048577,\n",
       " 1048578,\n",
       " 1048579,\n",
       " 1048582,\n",
       " 1048583,\n",
       " 1048584,\n",
       " 1048585,\n",
       " 1048586,\n",
       " 1048587]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "giant_set = [] # set of all non-duplicated articles\n",
    "for category, articles in reduce_categories.items():\n",
    "    giant_set.extend(articles)\n",
    "giant_set = list(set(giant_set))\n",
    "print(len(giant_set))\n",
    "giant_set[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we choose \"Year_of_death_missing\" as the input due to the time factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year_of_death_missing\n"
     ]
    }
   ],
   "source": [
    "C0 = input() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1048576,\n",
       " 1048577,\n",
       " 1048578,\n",
       " 1048579,\n",
       " 1261874,\n",
       " 720903,\n",
       " 892935,\n",
       " 892937,\n",
       " 892938,\n",
       " 1048587]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C0_list = reduce_categories.get(C0)\n",
    "C0_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-created bfs for a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(adjacency_dict,start,goal_list):\n",
    "    # dict which holds parents, later helpful to retrieve path.\n",
    "    # Also useful to keep track of visited node\n",
    "    parent = defaultdict(lambda:(False, np.inf))\n",
    "\n",
    "    parent[start] = (True,0)   \n",
    "    queue = deque([start])\n",
    "    while queue:\n",
    "        currNode = queue.popleft()\n",
    "        for child in adjacency_dict[currNode]:\n",
    "            if child not in parent: # if not visited\n",
    "                parent[child] = (True,parent[currNode][1]+1)\n",
    "                queue.append(child)\n",
    "    distance_list = []\n",
    "    for article in goal_list:\n",
    "        distance_list.append(parent[article][1])\n",
    "    return distance_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check our function's performance with the default nx.shortest_path_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nx.shortest_path_length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 8, inf, inf, 10, 11, 9, 9, inf, 10]\n",
      "[8, 7, inf, inf, 9, 10, 8, 8, inf, 9]\n",
      "[7, 6, inf, inf, 8, 9, 7, 7, inf, 8]\n",
      "[inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]\n",
      "[6, 5, inf, inf, 6, 7, 6, 6, inf, 6]\n",
      "[5, 4, inf, inf, 6, 7, 6, 5, inf, 6]\n",
      "[inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]\n",
      "[inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]\n",
      "[inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]\n",
      "[6, 5, inf, inf, 7, 8, 6, 6, inf, 7]\n"
     ]
    }
   ],
   "source": [
    "Ci_list = reduce_categories.get('Living_people')\n",
    "category = 'Living_people'\n",
    "for article in C0_list[:10]:\n",
    "    bfs_path = []\n",
    "    for article1 in Ci_list[:10]:\n",
    "        try:\n",
    "            distance = nx.shortest_path_length(G,article,article1) # bfs distance\n",
    "            bfs_path.append(distance)  # a list of shortest paths\n",
    "        except: # when there is not a path\n",
    "            bfs_path.append(np.inf)\n",
    "    print(bfs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our own bfs function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 8, inf, inf, 10, 11, 9, 9, inf, 10]\n",
      "[8, 7, inf, inf, 9, 10, 8, 8, inf, 9]\n",
      "[7, 6, inf, inf, 8, 9, 7, 7, inf, 8]\n",
      "[inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]\n",
      "[6, 5, inf, inf, 6, 7, 6, 6, inf, 6]\n",
      "[5, 4, inf, inf, 6, 7, 6, 5, inf, 6]\n",
      "[inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]\n",
      "[inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]\n",
      "[inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]\n",
      "[6, 5, inf, inf, 7, 8, 6, 6, inf, 7]\n"
     ]
    }
   ],
   "source": [
    "category = 'Living_people'\n",
    "Ci_list = reduce_categories.get('Living_people')\n",
    "for article in C0_list[:10]:\n",
    "    bfs_path = bfs(adj_dict,article,Ci_list) # bfs distance\n",
    "    print(bfs_path[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting unnecessary variables to save the memory, as we know that the next steps will be very memory-consuming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bfs_path, distance, categories, graph, connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from above our function is working fine. Now we work on a very very big dictionary. In order to do so we save it chunk by chunk we define the size the save the files for later computation of median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_distance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "size = 0\n",
    "for article in C0_list:\n",
    "    size += 1\n",
    "    bfs_path = bfs(adj_dict,article,giant_set) # bfs distance on whole list \n",
    "    node_distance[article] = bfs_path # a dict of shortest paths\n",
    "    if (size%300) == 0:\n",
    "        print(size)        \n",
    "        f = open(\"node_distance\"+str(size)+\".pkl\",\"wb\")\n",
    "        pickle.dump(node_distance,f)\n",
    "        f.close()\n",
    "        node_distance = {} # reset our dict\n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the last items for our dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "node_distance = {}\n",
    "for article in C0_list[3600:]:\n",
    "    bfs_path = bfs(adj_dict,article,giant_set) # bfs distance on whole list \n",
    "    node_distance[article] = bfs_path # a dict of shortest paths\n",
    "    f = open(\"node_distance\"+str(len(C0_list))+\".pkl\",\"wb\")\n",
    "    pickle.dump(node_distance,f)\n",
    "    f.close()\n",
    "node_distance = {} # reset our dict\n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we merged all the list of articles in the previous steps for faster computation of Breadth first search, now we have to create a dictionary to record down the positions of articles for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2380.6984961032867\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "index = {} # a dictionary records down positions of the articles\n",
    "for k, v in reduce_categories.items():\n",
    "    num_list = []\n",
    "    for i in v:\n",
    "        num_list.append(giant_set.index(i))\n",
    "    index[k] = num_list\n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the file for future usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"index.pkl\",\"wb\")\n",
    "pickle.dump(index,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can find the median and arrange the block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3997.3821783065796\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "files = ['300','600','900','1200','1500','1800',\n",
    "         '2100','2400','2700','3000','3300','3600','3629'] #get all the files \n",
    "\n",
    "for key, value in index.items(): # key gives the list name, value is the indeces\n",
    "    if key == C0:                # if key is the input\n",
    "        median_list.append((C0,0.0))\n",
    "    else:    \n",
    "        sub_list = []\n",
    "        for file in files:\n",
    "            with open(\"node_distance\"+file+\".pkl\", 'rb') as f:\n",
    "                node_distance = pickle.load(f)    \n",
    "                for k, v in node_distance.items(): # v gives distances, k gives articles in input        \n",
    "                    for i in value:\n",
    "                        sub_list.append(v[i])\n",
    "        # calculate median for each category\n",
    "        median = statistics.median(sub_list)\n",
    "        component = (key,median)\n",
    "        median_list.append(component)\n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"median_list.pkl\",\"wb\")\n",
    "pickle.dump(median_list,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('median_list.pkl', 'rb') as f:\n",
    "    median_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('English_footballers', inf),\n",
       " ('The_Football_League_players', 11.0),\n",
       " ('Association_football_forwards', inf),\n",
       " ('Association_football_goalkeepers', inf),\n",
       " ('Association_football_midfielders', inf),\n",
       " ('Association_football_defenders', inf),\n",
       " ('Living_people', inf),\n",
       " ('Harvard_University_alumni', 15.0),\n",
       " ('Major_League_Baseball_pitchers', inf),\n",
       " ('Members_of_the_United_Kingdom_Parliament_for_English_constituencies', 7.0),\n",
       " ('Indian_films', 7.0),\n",
       " ('Year_of_death_missing', 0.0),\n",
       " ('Year_of_birth_missing_(living_people)', inf),\n",
       " ('Rivers_of_Romania', 8),\n",
       " ('Main_Belt_asteroids', inf),\n",
       " ('Asteroids_named_for_people', inf),\n",
       " ('English-language_albums', 7.0),\n",
       " ('British_films', 6.0),\n",
       " ('English-language_films', 6.0),\n",
       " ('American_films', 6.0),\n",
       " ('People_from_New_York_City', 7.0),\n",
       " ('American_television_actors', 6),\n",
       " ('American_film_actors', 6.0),\n",
       " ('Debut_albums', 8.0),\n",
       " ('Black-and-white_films', 6.0),\n",
       " ('Year_of_birth_missing', inf),\n",
       " ('Place_of_birth_missing_(living_people)', 9.0),\n",
       " ('American_military_personnel_of_World_War_II', inf),\n",
       " ('Windows_games', 9.0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_list    # take a look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Year_of_death_missing', 0.0),\n",
       " ('British_films', 6.0),\n",
       " ('English-language_films', 6.0),\n",
       " ('American_films', 6.0),\n",
       " ('American_television_actors', 6),\n",
       " ('American_film_actors', 6.0),\n",
       " ('Black-and-white_films', 6.0),\n",
       " ('Members_of_the_United_Kingdom_Parliament_for_English_constituencies', 7.0),\n",
       " ('Indian_films', 7.0),\n",
       " ('English-language_albums', 7.0),\n",
       " ('People_from_New_York_City', 7.0),\n",
       " ('Rivers_of_Romania', 8),\n",
       " ('Debut_albums', 8.0),\n",
       " ('Place_of_birth_missing_(living_people)', 9.0),\n",
       " ('Windows_games', 9.0),\n",
       " ('The_Football_League_players', 11.0),\n",
       " ('Harvard_University_alumni', 15.0),\n",
       " ('English_footballers', inf),\n",
       " ('Association_football_forwards', inf),\n",
       " ('Association_football_goalkeepers', inf),\n",
       " ('Association_football_midfielders', inf),\n",
       " ('Association_football_defenders', inf),\n",
       " ('Living_people', inf),\n",
       " ('Major_League_Baseball_pitchers', inf),\n",
       " ('Year_of_birth_missing_(living_people)', inf),\n",
       " ('Main_Belt_asteroids', inf),\n",
       " ('Asteroids_named_for_people', inf),\n",
       " ('Year_of_birth_missing', inf),\n",
       " ('American_military_personnel_of_World_War_II', inf)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_list.sort(key=itemgetter(1))\n",
    "median_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Median_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Year_of_death_missing</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>British_films</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English-language_films</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>American_films</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>American_television_actors</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>American_film_actors</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Black-and-white_films</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Members_of_the_United_Kingdom_Parliament_for_E...</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indian_films</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>English-language_albums</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>People_from_New_York_City</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Rivers_of_Romania</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Debut_albums</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Place_of_birth_missing_(living_people)</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Windows_games</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The_Football_League_players</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Harvard_University_alumni</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>English_footballers</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Association_football_forwards</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Association_football_goalkeepers</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Association_football_midfielders</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Association_football_defenders</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Living_people</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Major_League_Baseball_pitchers</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Year_of_birth_missing_(living_people)</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Main_Belt_asteroids</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Asteroids_named_for_people</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Year_of_birth_missing</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>American_military_personnel_of_World_War_II</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Category  Median_Score\n",
       "0                               Year_of_death_missing      0.000000\n",
       "1                                       British_films      6.000000\n",
       "2                              English-language_films      6.000000\n",
       "3                                      American_films      6.000000\n",
       "4                          American_television_actors      6.000000\n",
       "5                                American_film_actors      6.000000\n",
       "6                               Black-and-white_films      6.000000\n",
       "7   Members_of_the_United_Kingdom_Parliament_for_E...      7.000000\n",
       "8                                        Indian_films      7.000000\n",
       "9                             English-language_albums      7.000000\n",
       "10                          People_from_New_York_City      7.000000\n",
       "11                                  Rivers_of_Romania      8.000000\n",
       "12                                       Debut_albums      8.000000\n",
       "13             Place_of_birth_missing_(living_people)      9.000000\n",
       "14                                      Windows_games      9.000000\n",
       "15                        The_Football_League_players     11.000000\n",
       "16                          Harvard_University_alumni     15.000000\n",
       "17                                English_footballers           inf\n",
       "18                      Association_football_forwards           inf\n",
       "19                   Association_football_goalkeepers           inf\n",
       "20                   Association_football_midfielders           inf\n",
       "21                     Association_football_defenders           inf\n",
       "22                                      Living_people           inf\n",
       "23                     Major_League_Baseball_pitchers           inf\n",
       "24              Year_of_birth_missing_(living_people)           inf\n",
       "25                                Main_Belt_asteroids           inf\n",
       "26                         Asteroids_named_for_people           inf\n",
       "27                              Year_of_birth_missing           inf\n",
       "28        American_military_personnel_of_World_War_II           inf"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(median_list,columns= ['Category','Median_Score'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting the nodes in each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Computing the sub_graph induced by C0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "C0_graph = {}\n",
    "C0_set = set(C0_list)\n",
    "for article in C0_list:\n",
    "    temp = set(G.predecessors(article)) # return predecessors of a node in the graph\n",
    "    score = len(temp.intersection(C0_set))\n",
    "    C0_graph[article] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have C0 sub-graph of format: {article1:score,article2:score....} <br/>\n",
    "We are now moving to step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2:  Extending the graph to the nodes that belong to C1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we notice, this step is consisted of two sub-steps. First one is to compute the in-edge scores of the nodes in C1, the second one is to compute the total scores by adding the scores in sub-step 1 to the inter-edge scores from nodes in C0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'British_films'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C1 = median_list[1][0]\n",
    "C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1056769,\n",
       " 1056770,\n",
       " 1056774,\n",
       " 1245190,\n",
       " 1064970,\n",
       " 1040395,\n",
       " 1056780,\n",
       " 1056787,\n",
       " 1056788,\n",
       " 811030]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C1_list = reduce_categories.get(C1)\n",
    "C1_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first sub-step\n",
    "C1_graph = {}\n",
    "C1_set = set(C1_list)\n",
    "for article in C1_list:\n",
    "    temp = set(G.predecessors(article))\n",
    "    score = len(temp.intersection(C1_set))\n",
    "    C1_graph[article] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second sub_step\n",
    "for article, score in C1_graph.items():\n",
    "    new_score = score\n",
    "    temp = set(G.predecessors(article))\n",
    "    predecessors = temp.intersection(C0_set)\n",
    "    for i in predecessors:\n",
    "        new_score += C0_graph.get(i)\n",
    "    C1_graph[article] = new_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the ranking of C1 by the highest score first and going down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1041937, 1501),\n",
       " (1253712, 15),\n",
       " (1253703, 14),\n",
       " (1253707, 13),\n",
       " (1253709, 13),\n",
       " (1253706, 11),\n",
       " (1253711, 8),\n",
       " (1044136, 8),\n",
       " (1061229, 8),\n",
       " (1253690, 7)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_ = sorted(C1_graph.items(), key=itemgetter(1), reverse = True)\n",
    "sorted_[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems a little bit odd when the article 1041937 has such a high score. However when we check the predecessors of 1041937 the case is well-explained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1654"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(G.predecessors(1041937)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a lot of that predecessors should come from the same category, such they give many ones to the score of 1041937!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3:  Repeating Step2 up to the last category of the ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = {**C0_graph,**C1_graph} # joining the first two graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8011"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(score_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English-language_films\n",
      "American_films\n",
      "American_television_actors\n",
      "American_film_actors\n",
      "Black-and-white_films\n",
      "Members_of_the_United_Kingdom_Parliament_for_English_constituencies\n",
      "Indian_films\n",
      "English-language_albums\n",
      "People_from_New_York_City\n",
      "Rivers_of_Romania\n",
      "Debut_albums\n",
      "Place_of_birth_missing_(living_people)\n",
      "Windows_games\n",
      "The_Football_League_players\n",
      "Harvard_University_alumni\n",
      "English_footballers\n",
      "Association_football_forwards\n",
      "Association_football_goalkeepers\n",
      "Association_football_midfielders\n",
      "Association_football_defenders\n",
      "Living_people\n",
      "Major_League_Baseball_pitchers\n",
      "Year_of_birth_missing_(living_people)\n",
      "Main_Belt_asteroids\n",
      "Asteroids_named_for_people\n",
      "Year_of_birth_missing\n",
      "American_military_personnel_of_World_War_II\n"
     ]
    }
   ],
   "source": [
    "for i in range(2,len(median_list)):\n",
    "    Ci = median_list[i][0]\n",
    "    print(Ci)                          # checking the progress\n",
    "    Ci_list = reduce_categories.get(Ci)\n",
    "    \n",
    "    set_dict = set(score_dict.keys())\n",
    "    # sub-steps\n",
    "    Ci_graph = {}\n",
    "    Ci_set = set(Ci_list)\n",
    "    for article in Ci_list:\n",
    "        temp = set(G.predecessors(article))\n",
    "        score = len(temp.intersection(Ci_set))\n",
    "        predecessors = temp.intersection(set_dict)\n",
    "        for i in predecessors:\n",
    "            score += score_dict.get(i)\n",
    "        Ci_graph[article] = score\n",
    "    score_dict = {**score_dict,**Ci_graph}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422144"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(score_dict.keys()) # checking the keys whether it's matched total number of articles or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the top articles with the highest scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1400483, 477283325),\n",
       " (1400478, 453830224),\n",
       " (1061812, 450594335),\n",
       " (1400484, 397007898),\n",
       " (1061684, 382551609),\n",
       " (1062022, 323957782),\n",
       " (1163692, 288820905),\n",
       " (1061683, 272231405),\n",
       " (1163560, 222802986),\n",
       " (1061441, 217284790)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_ = sorted(score_dict.items(), key=itemgetter(1), reverse = True)\n",
    "sorted_[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we return the node ranking inside the sub-graph by the name of the articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1400483 Jimmy Carter',\n",
       " 477283325,\n",
       " '1400478 John F. Kennedy',\n",
       " 453830224,\n",
       " '1061812 Paul Newman',\n",
       " 450594335,\n",
       " '1400484 Richard Nixon',\n",
       " 397007898,\n",
       " '1061684 Jack Lemmon',\n",
       " 382551609,\n",
       " '1062022 Robert Altman',\n",
       " 323957782,\n",
       " '1163692 Dean Martin',\n",
       " 288820905,\n",
       " '1061683 Walter Matthau',\n",
       " 272231405,\n",
       " '1163560 Johnny Carson',\n",
       " 222802986,\n",
       " '1061441 Charlton Heston',\n",
       " 217284790]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking = []\n",
    "for i in sorted_:\n",
    "    attribute = (pagenames[i[0]], i[1])\n",
    "    ranking.extend(attribute)\n",
    "ranking[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is quite a striking result for node ranking, considering that we take 'Year_of_death_missing' as the input and now we have pages of famous people as top results !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
